---
documentclass: jss
author:
  - name: SÃ¸ren B. Vilsen
    affiliation: |
      | Department of Mathematical Sciences, Aalborg University
    address: |
      | Skjernvej 4A,
      | 9220 Aalborg East
    email: \email{svilsen@math.aau.dk}
    url: people.math.aau.dk/~svilsen
address: Department of Mathematical Sciences, Aalborg University
title:
  formatted: "Random weight neural networks in \\proglang{R}: The \\pkg{RWNN} package"
  plain:     "Random weight neural networks in R: The RWNN package"
  short:     "\\pkg{RWNN}: Random Weight Neural Networks"
abstract: >
  This paper serves as an introduction to the \pkg{RWNN} package. The \pkg{RWNN} package implements random weight neural networks. The methods are implemented using a combination of \proglang{R} and \proglang{C++} offsetting the heavier computation and estimation to \proglang{C++}  through the \pkg{Rcpp} and \pkg{RcppArmadillo} packages. While implementations of random weight neural networks exist other \proglang{R} packages, these focus on the simplest possible variant of the random weight neural network and cover only very specialised use cases of these networks. Besides a general purpose implementation of random weight neural networks, the \pkg{RWNN} package also includes common variants such as deep RWNN and sparse RWNN, as well as ensemble methods using random weight neural networks as the base learner.
keywords:
  formatted: [random weight neural networks, regularisation, ensemble learning, Bayesian computation, "\\proglang{R}", "\\pkg{Rcpp}", "\\pkg{RcppArmadillo}"]
  plain:     [random weight neural networks, regularisation, ensemble learning, Bayesian computation, R, Rcpp, RcppArmadillo]
output:
  rticles::jss_article:
    keep_tex: TRUE
    number_sections: TRUE
    citation_package: "natbib"
    includes:
        in_header: preamble.tex
biblio-style: jss
bibliography: literature.bib
---

```{r setup, include = FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction

Neural networks, and variants thereof, have seen a massive increase in popularity in recent years. This has largely been due to the flexibility of the neural network architecture, and their accuracy when applied to highly non-linear problems. However, due to the highly non-linear nature of the neural network architecture, estimating the weights of these networks using gradient based optimisation (i.e. back-propagation), can be slow and does not guarantee a globally optimal solution. In order to combat these problems, various simplifications of the feed forward neural network (FFNN) architecture have been proposed, including random weight neural networks (RWNNs). They were first introduced in the early 1990's under the name random vector functional links (RVFL) [@Schmidt1992, @Pao1994], and a simplified version was re-discovered under the name extreme learning machines (ELM) [@Huang2006] in the mid 2000's. The general idea of RWNNs is to keep the randomly assigned weights of the network between the input-layer and the last hidden-layer fixed, and focus on estimating the weights between the last hidden-layer and the output-layer. In the case of regression, this simplification makes estimating the output weights of a RWNN equivalent to estimating the weights of a (regularised) linear model. Theoretically RWNNs show similar universal approximation properties as their FFNN counterparts, i.e. as the number of neurons tends towards infinity the RWNN should be able to approximate any function arbitrarily well (placing only loose assumptions on the activation function). However, practically the number of neurons needed for this approximation to be acceptable may not be feasible for a particular application. Therefore, extensions of RWNNs have been proposed limiting the number of neurons in favour of deeper architecture, as seen in deep RWNN [@Henriquez2018] and ensemble deep RWNN [@Shi2021], or in favour of sparse unsupervised pre-training of the weights, like sparse RWNN [@Zhang2019].

Implementations of RWNNs already exist in \proglang{R} through the packages \pkg{nnfor} [@nnfor] and \pkg{elmNNRcpp} [@elmNNRcpp], both focusing on the simpler ELM architecture. Both implementations allow for a varying number of neurons in the hidden-layer, as well as the specification of the activation function, but are limited to a single hidden-layer with no functional link between the input- and output-layers. The \pkg{nnfor} package was designed for using ELMs to handle time-series data and allows for forecasting at different temporal frequencies using the \pkg{thief} package. Furthermore, it allows for estimation of the output-weights using Moore-Penrose inversion, $\ell_1$-regularisation, and $\ell_2$-regularisation. The \pkg{elmNNRcpp} package is the successor to \pkg{elmNN} package [@elmNN] re-implemented in \proglang{C++} through \pkg{Rcpp} and \pkg{RcppArmadillo} [@Rcpp, @RcppA]. The package is a standard implementation of the ELM architecture for both regression and classification. The package allows the user to specify the leakage of the implemented relu activation function, as well as the tolerance of the Moore-Penrose inversion used to estimate the output-weights.

\pkg{RWNN} is a general purpose implementation of RWNNs in \proglang{R} [@R] focusing on regression problems. The \pkg{RWNN} package allows the user to create an RWNN of any depth, set the number of neurons and activation functions in each layer, choose the sampling distribution of the randomly assigned weights, and choose whether the output weights should be estimated by either Moore-Penrose inversion, $\ell_1$-regularisation, or $\ell_2$-regularisation. The RWNN is implemented in C++ through \pkg{Rcpp} and \pkg{RcppArmadillo}, and along with the standard RWNN implementation the following variants have also been included:

 - **ELM** (extreme learning machine) [@Huang2006]: A simplified version of an RWNN without a link between the input and output layer.
 - **deep RWNN** [@Henriquez2018]: An RWNN with multiple hidden layers, where the output of each hidden-layer is included as features in the model.
 - **sparse RWNN** [@Zhang2019]: Applies sparse auto-encoder ($\ell_1$ regularised) pre-training to reduce the number non-zero weights between the input and the hidden layer (the implementation generalises this concept to allow for both $\ell_1$ and $\ell_2$ regularisation).
 - **ensemble deep RWNN** [@Shi2021]: An extension of deep RWNNs using the output of each hidden layer to create separate RWNNs. These RWNNs are then used to create an ensemble prediction of the target.  

Furthermore, the \pkg{RWNN} package also includes general implementations of the following ensemble methods (using RWNNs as base learners):

 - **Stacking**: Stack multiple randomly generated RWNN's, and estimate their contribution to the weighted ensemble prediction using $k$-fold cross-validation.
 - **Bagging** [@Xin2021]: Bootstrap aggregation of RWNN's creates a number of bootstrap samples, sampled with replacement from the training-set. Furthermore, as in random forest, instead of using all features when training each RWNN, a subset of the features can be  chosen at random.
 - **Boosting**: Gradient boosting creates a series of RWNN's, where an element, $k$, of the series is trained on the residual of the previous $(k - 1)$ RWNN's. It further allows for manipulation of the learning rate used to improve the generalisation of the boosted model. Lastly, like the implemented bagging method, the number of features used in each iteration can be chosen at random (also called stochastic gradient boosting).

Lastly, the \pkg{RWNN} package also includes a simple method for grid based hyperparameter optimisation, where $k$-fold cross-validation is applied to training-set to find the hyperparameters yielding the smallest cross-validation error on a supplied grid.

The remainder of the paper is structured as follows: Section \ref{RWNN} introduces the general idea of RWNNs, this is followed by the method of estimating the output weights, and an outline of each of the implemented RWNN variants. The utility of the \pkg{RWNN} package is shown in Section \ref{EX} applying different RWNN variants to a series of examples. Lastly, a conclusion is found in Section \ref{CON}.


# Random Weight Neural Networks {#RWNN}

The general structure of an RWNN with a single hidden-layer can be seen in Figure \ref{fig:rwnn}. The RWNN is a simplification of a simple FFNN, where the weights between the input-layer and the hidden-layer are kept fixed after the randomly initialisation of the network. That is, the weights between the last hidden-layer and the output-layer are the only weights estimated during the training process. Furthermore, an RWNN may include a direct (also called functional) link between the features and the output, shown as dashed lines in Figure \ref{fig:rwnn}. When these links are active, the output can be seen as a concatenation of the last hidden-layer and the input-layer (i.e. a concatenation of the both the original and random non-linear transformation of the features). Using this simplification, the estimation of the output-weights simplifies greatly if the activation between the concatenated layer and the output is linear (i.e. the identity function), as estimating of the output-weights becomes equivalent to estimating the weights in a (regularised) multiple linear regression.

```{r rwnn, out.width = "0.6\\linewidth", include = TRUE, fig.align = "center", fig.cap=c("Graph representation of a random weight neural network (RWNN) with funcitonal link between the input and the output layer."), echo=FALSE}
knitr::include_graphics("./Figures/RWNN.pdf")
```

Given a sample of $N$ observations, $\mathcal D = \{(\boldsymbol x_n, y_n)\}_{n = 1}^N$, where $\boldsymbol x_n$ is $p$ dimensional vector of features and $y_n$ is the output of observation $n$, then the output of the $j$'th neuron of the hidden layer:
\begin{equation}
h_{nj} = f\left(\sum_{i = 1}^p w_{ij} x_{ni} + w_{0}\right), \label{eq:hidden}
\end{equation}
$f$ is the activation function, $w_0$ is the bias, and $w_{ij}$ is the weight between the $i$'th feature and the $j$'th neuron in $n$'th observation. If the hidden-layer contains $J$ neurons the vector, the vector of transformed features of the hidden layer, denoted $\boldsymbol h_{n}$, simplifying the notation of Eq. \eqref{eq:hidden} to:
\begin{equation}
\boldsymbol h_n = \boldsymbol f\left(W \boldsymbol{x}_n + w_0 \boldsymbol{1}_{J}\right),
\end{equation}
where $\boldsymbol{1}_{J}$ is a vector of one's of dimension $J$.

Let $\boldsymbol d_n = [\boldsymbol h_n \; \boldsymbol x_n]^T$ be a stacked vector of the features transformed by the hidden-layer, and original features. Furthermore, assuming the activation in the output-layer is linear, then the expected response of the $n$'th observation is:
\begin{equation}
\hat{y}_n = \boldsymbol d^T_n \boldsymbol \beta.
\end{equation}

That is, if weights of the hidden-layer can be assumed to be known, then the expected response of an RWNN is identical to that of a linear model. Three approaches to estimating output-weights can be found in Section [@EST], however, first it is necessary to decide how to determine the hidden-weights.

## Determining hidden-weights


## Estimating output-weights {#EST}
Given a concatenated matrix of features, $D$, (i.e. a matrix whose $n$'th row contains $\boldsymbol d_n$) and $\boldsymbol y$ be a vector containing the output, the output-weights $\boldsymbol \beta$ can be found by minimising sum-of-squared errors (SSE):
\begin{equation}
\hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{argmin}} \Big\{ || \boldsymbol{y} - D\boldsymbol{\beta}||_2^2\Big\}. \label{eq:ssq}
\end{equation}

When $N > p + J$, the solution to this system of equations can be found by the normal equation:
\begin{equation}
\hat{\boldsymbol{\beta}}^{(ols)} = (D^T D)^{-1}D^T\boldsymbol{y}. \label{eq:ols}
\end{equation}

However, in many application the number of concatenated features (i.e. the number of columns of $D$) is much bigger than the number of observations, i.e. $(p + J) >> N$. When this is the case, the two most common approaches to estimating the parameters are the Moore-Penrose pseudoinverse and regularisation.

When using Moore-Penrose pseudoinverse [@BjerhammarInv, @PenroseInv], $D^+$, the solution to the optimisation problem is simply:
\begin{equation}
\hat{\boldsymbol \beta}^{(pseudo)} = D^+ \boldsymbol y.
\end{equation}

When regularisation is introduced the SSE, seen in Eq.\ \eqref{eq:ssq}, is penalised by the size of the output-weights. The penalisation is usually performed using $\ell_1$ or $\ell_2$ norm, creating the following optimisation problem:
\begin{equation}
\hat{\boldsymbol{\beta}}^{(reg)} = \underset{\boldsymbol{\beta}}{\text{argmin}} \Big\{ || \boldsymbol{y} - D\boldsymbol{\beta}||_2^2  + \lambda||\boldsymbol{\beta}||_q^q \Big\}, \label{eq:rsse}
\end{equation}
where $q \in \{1, 2\}$ and $\lambda$ is a penalisation constant. The penalisation constant should be chosen such that it minimises the out-of-sample error (using e.g. $k$-fold cross-validation during training process).

If $q = 2$, the optimisation problem in Eq. \eqref{eq:rsse} is equivalent to performing ridge-regression on the concatenated features, instead of the input features, and the solution can be found as [@ridgeReg]:
\begin{equation}
\hat{\boldsymbol \beta}^{(ridge)} = \left(D^TD + \lambda I_{p + J}\right)^{-1}D^T\boldsymbol y,
\end{equation}
where $I_{p+J}$ is the identity matrix of size $p+J$.

If $q = 1$, the optimisation problem in Eq. \eqref{eq:rsse} is equivalent to lasso regression [@SanLasso, @TibLasso]. Unlike ridge-regression it is not possible to find a closed form solution of the lasso estimated output-weights, $\hat{\boldsymbol{\beta}}^{(lasso)}$, however, they can be found by using coordinate descent [@CoordLasso].

**NB:** Extreme learning machine's (ELM's) are effectively RWNN's without the functional link and can, therefore, be trained using exactly the same set of equations by setting $\boldsymbol d_n = \boldsymbol h_n$.

## Deep RWNN


\begin{equation}
h_{nj}^{(k)} = f^{(k)}\left(\sum_{i = 1}^p w^{(k)}_{ij} h_{ni}^{(k - 1)} + w^{(k)}_{0}\right), \label{eq:hidden2}
\end{equation}
for $k = 1, ..., K$, where $h_{ni}^{(0)}$ is feature $x_{ni}$, $f^{(k)}$ is the activation function, $w^{(k)}_0$ is the bias, and $w^{(k)}_{ij}$ is the weight between the $i$'th feature and the $j$'th neuron in the $k$'th layer. If the hidden-layer contains $J^{(k)}$ neurons the vector, the vector of transformed features of the $k$'th layer is denoted $\boldsymbol h^{(k)}_{n}$ simplifying the notation of Eq. \eqref{eq:hidden} to:
\begin{equation}
\boldsymbol h^{(k)} = \boldsymbol f^{(k)}\left(W^{(k)} \boldsymbol{h}^{(k)} + w^{(k)}_0 \boldsymbol{1}\right),
\end{equation}
where $\boldsymbol{1}$ is a vector of one's of dimension $J^{k}$.

Let $\boldsymbol d_n = [\boldsymbol h^{(K)}_n \; \boldsymbol x_n]^T$ be a stacked vector of the features transformed by the $K$ hidden-layers, and original features. Furthermore, assuming the activation in the output-layer is linear, then the predicted response of the $n$'th observation is:
\begin{equation}
\hat{y}_n = \boldsymbol d^T_n \boldsymbol \beta
\end{equation}

## Sparse RWNN

## Ensemble methods

### Stacking

### Bagging

### Boosting

## Ensemble Deep RWNN


# Examples {#EX}


# Conclusions {#CON}


# References

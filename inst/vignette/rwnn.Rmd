---
documentclass: jss
author:
  - name: SÃ¸ren B. Vilsen
    affiliation: |
      | Department of Mathematical Sciences, Aalborg University
    address: |
      | Skjernvej 4A,
      | 9220 Aalborg East
    email: \email{svilsen@math.aau.dk}
    url: people.math.aau.dk/~svilsen
address: Department of Mathematical Sciences, Aalborg University
title:
  formatted: "Random weight neural networks in \\proglang{R}: The \\pkg{RWNN} package"
  plain:     "Random weight neural networks in R: The RWNN package"
  short:     "\\pkg{RWNN}: Random Weight Neural Networks"
abstract: >
  This paper serves as an introduction to the \pkg{RWNN} package. The \pkg{RWNN} package implements random weight neural networks. The methods are implemented using a combination of \proglang{R} and \proglang{C++} offsetting the heavier computation and estimation to \proglang{C++}  through the \pkg{Rcpp} and \pkg{RcppArmadillo} packages. While implementations of random weight neural networks exist other \proglang{R} packages, these focus on the simplest possible variant of the random weight neural network and cover only very specialised use cases of these networks. Besides a general purpose implementation of random weight neural networks, the \pkg{RWNN} package also includes common variants such as deep RWNN and sparse RWNN, as well as ensemble methods using random weight neural networks as the base learner. Furhtermore, the \pkg{RWNN} packages also includes more sophisticated methods for initialising the random weights, as well as methods for pruning both the number of weights and the number of neurons in the network.
keywords:
  formatted: [random weight neural networks, regularisation, ensemble learning, pruning, "\\proglang{R}", "\\pkg{Rcpp}", "\\pkg{RcppArmadillo}"]
  plain:     [random weight neural networks, regularisation, ensemble learning, pruning, R, Rcpp, RcppArmadillo]
output:
  rticles::jss_article:
    keep_tex: TRUE
    number_sections: TRUE
    citation_package: "natbib"
    includes:
        in_header: preamble.tex
biblio-style: jss
bibliography: literature.bib
---

```{r setup, include = FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction

Neural networks, and variants thereof, have seen a massive increase in popularity in recent years. This has largely been due to the flexibility of the neural network architecture, and their accuracy when applied to highly non-linear problems. However, due to the highly non-linear nature of the neural network architecture, estimating the weights of these networks using gradient based optimisation (i.e. back-propagation), can be slow and does not guarantee a globally optimal solution. In order to combat these problems, various simplifications of the feed forward neural network (FFNN) architecture have been proposed, including random weight neural networks (RWNNs). They were first introduced in the early 1990's under the name random vector functional links (RVFL) [@Schmidt1992, @Pao1994], and a simplified version was re-discovered under the name extreme learning machines (ELM) [@Huang2006] in the mid 2000's. The general idea of RWNNs is to keep the randomly assigned weights of the network between the input-layer and the last hidden-layer fixed, and focus on estimating the weights between the last hidden-layer and the output-layer. In the case of regression, this simplification makes estimating the output weights of a RWNN equivalent to estimating the weights of a (regularised) linear model. Theoretically RWNNs show similar universal approximation properties as their FFNN counterparts, i.e. as the number of neurons tends towards infinity the RWNN should be able to approximate any function arbitrarily well (placing only loose assumptions on the activation function). However, practically the number of neurons needed for this approximation to be acceptable may not be feasible for a particular application. Therefore, extensions of RWNNs have been proposed limiting the number of neurons in favour of deeper architecture, as seen in deep RWNN [@Henriquez2018] and ensemble deep RWNN [@Shi2021], or in favour of sparse unsupervised pre-training of the weights, like sparse RWNN [@Zhang2019].

Implementations of RWNNs already exist in \proglang{R} through the packages \pkg{nnfor} [@nnfor] and \pkg{elmNNRcpp} [@elmNNRcpp], both focusing on the simpler ELM architecture. Both implementations allow for a varying number of neurons in the hidden-layer, as well as the specification of the activation function, but are limited to a single hidden-layer with no functional link between the input- and output-layers. The \pkg{nnfor} package was designed for using ELMs to handle time-series data and allows for forecasting at different temporal frequencies using the \pkg{thief} package. Furthermore, it allows for estimation of the output-weights using Moore-Penrose inversion, $\ell_1$-regularisation, and $\ell_2$-regularisation. The \pkg{elmNNRcpp} package is the successor to \pkg{elmNN} package [@elmNN] re-implemented in \proglang{C++} through \pkg{Rcpp} and \pkg{RcppArmadillo} [@Rcpp, @RcppA]. The package is a standard implementation of the ELM architecture for both regression and classification. The package allows the user to specify the leakage of the implemented relu activation function, as well as the tolerance of the Moore-Penrose inversion used to estimate the output-weights.

\pkg{RWNN} is a general purpose implementation of RWNNs in \proglang{R} [@R] focusing on both regression and classification problems. The \pkg{RWNN} package allows the user to create an RWNN of any depth, set the number of neurons and activation functions in each layer, choose the sampling distribution of the randomly assigned weights, and choose whether the output weights should be estimated by either Moore-Penrose inversion, $\ell_1$-regularisation, or $\ell_2$-regularisation. RWNN is implemented in C++ through \pkg{Rcpp} and \pkg{RcppArmadillo}; along with the standard RWNN implementation the following variants have also been included:

-   **ELM** (extreme learning machine) [@Huang2006]: A simplified version of an RWNN without a link between the input and output layer (this is also the default behaviour of the RWNN implementation).
-   **deep RWNN** [@Henriquez2018, [@Shi2021]: An RWNN with multiple hidden layers, where the output of each hidden-layer is included as features in the model.
-   **sparse RWNN** [@Zhang2019]: Applies sparse auto-encoder ($\ell_1$ regularised) pre-training to reduce the number non-zero weights between the input and the hidden layer (the implementation generalises this concept to allow for both $\ell_1$ and $\ell_2$ regularisation).
-   **ensemble deep RWNN** [@Shi2021]: An ensemble extension of deep RWNNs using the output of each hidden layer to create an ensemble of RWNNs, each hidden-layer being used to create a seperate prediction of the target.

Furthermore, the \pkg{RWNN} package also includes general implementations of the following ensemble methods (using RWNNs as base learners):

-   **Stacking** [@Wolpert1992, @Breiman1996a]: Stack multiple randomly generated RWNN's, deep RWNN's, or sparse RWNN's and estimate their contribution to a weighted ensemble using $k$-fold cross-validation.
-   **Bagging** [@Breiman1996b, @Breiman2001, @Xin2021]: Bootstrap aggregation of RWNN's, deep RWNN's, or sparse RWNN's creates a number of bootstrap samples, sampled with replacement from the training-set. Furthermore, as in random forest, instead of using all features when training each RWNN, a subset of the features are chosen at random.
-   **Boosting** [@Friedman2001]: The boosting implementation is based on stochastic residual boosting using RWNN's, deep RWNN's, or sparse RWNN's as the base learner. 

Lastly, in order to improve computation and memory efficiency, the \pkg{RWNN} package includes the following methods for pruning the number of weights and neurons:

-   **Global magnitude** (weight pruning) [@han2016, @morcos2019]: Pruning a pre-defined proportion of the weights with lowest magnitude globally across the network.
-   **Uniform magnitude** (weight pruning) [@han2016, @morcos2019]: Pruning a pre-defined proportion of the weights with lowest magnitude layer-by-layer.
-   **LAMP** (weight pruning) [@Lee2021]: Pruning a pre-defined proportion of the weights with lowest LAMP (layer-adaptive magnitude-based pruning) score globally across the network. The LAMP score is essentially a re-scaled magnitude.
-   **APoZ** (neuron pruning) [@hu2016]: Pruning a pre-defined proportion of neurons in layers, activated by the ReLU activation function, based on the average proportion of zeros (APoZ).
-   **Correlation** (neuron pruning): Pruning based on the pair-wise correlations between neurons layer-by-layer. A neuron is removed if its correlation exceeds a pre-defined threshold, or if its correlation is not significantly smaller than this pre-defined threshold.
-   **Relief** (weight and neuron pruning) [@Dekhovich2024]: Pruning a pre-defined proportion of the weights that, on average, provide only a small contribution to the next layer. 

The remainder of the paper is structured as follows: Section \ref{RWNN} introduces the general idea of RWNNs, this is followed by the method of estimating the output weights, and an outline of each of the implemented RWNN variants. The utility of the \pkg{RWNN} package is shown in Section \ref{EX} applying different RWNN variants to a series of examples. Lastly, a conclusion is found in Section \ref{CON}.

# Random Weight Neural Networks {#RWNN}

RWNN's are simplifications of FFNN's, where the weights between the input-layer and the hidden-layers are kept fixed after being randomly initialised, i.e. only the weights between the last hidden-layer and the output-layer are estimated during the training process. Furthermore, a direct link (also called a functional link) between the features and the output may be included; when the link is active, the RWNN can be seen as a concatenation of the original features, and a random non-linear transformation of these features. Using this simplification, assuming the loss is the squared loss, and that the activation function is linear (i.e. the identity function), then estimation of the output-weights simplifies greatly, as it becomes equivalent to estimating the weights in (regularised) multiple linear regression. The general structure of RWNN's with a single hidden-layer can be seen in Figure \ref{fig:rwnn}, where the dashed red lines indicate the functional link.

```{r rwnn, out.width = "0.6\\linewidth", include = TRUE, fig.pos = "ht!", fig.align = "center", fig.cap=c("Graph representation of a random weight neural network (RWNN) with funcitonal link between the input and the output layer."), echo=FALSE}
knitr::include_graphics("./Figures/RWNN.pdf")
```

## RWNN {#rwnn}

Given a sample of $N$ observations, $\mathcal D = \{(\boldsymbol x_n, y_n)\}_{n = 1}^N$, where $\boldsymbol x_n$ is $p$ dimensional vector of features and $y_n$ is the target, of observation $n$, the output of the $j$'th neuron in the hidden layer is: \begin{equation}
    h_{nj} = f\left(\sum_{i = 1}^p w_{ij} x_{ni} + w_{0}\right), \label{eq:hidden}
\end{equation} where $f$ is the activation function, $w_0$ is the bias, and $w_{ij}$ is the weight between the $i$'th feature and the $j$'th neuron. If the hidden-layer contains $J$ neurons, then the vector of features transformed by the hidden layer, denoted $\boldsymbol h_{n}$, can be simplified to: \begin{equation} 
    \boldsymbol h_n = \boldsymbol f\left(W \boldsymbol{x}_n + w_0 \boldsymbol{1}_{J}\right), 
\end{equation} where $\boldsymbol{1}_{J}$ is a vector of one's of dimension $J$.

Assuming the activation in the output-layer is linear, then the expected response of the $n$'th observation is: \begin{equation}
    \hat{y}_n = \boldsymbol d^T_n \boldsymbol \beta,
\end{equation} where $\boldsymbol d_n$ is either the output of the hidden-layer (i.e.\ $\boldsymbol d_n = \boldsymbol h_n$), or the concatenation of input-layer and the output of the hidden-layer (i.e.\ $\boldsymbol d_n = [\boldsymbol h_n^T \; \boldsymbol x_n^T]^T$), dependent on whether the functional link is inactive or active, respectively. Furthermore, in the following, references to the the functional link will be avoided when possible, and the dimension of the features $\boldsymbol d_n$ will simply be denoted $l$.

The \pkg{RWNN} package allows the user to specify the number of neurons in the hidden-layer, and the activation function among those listed in Table \ref{tab:activation}.

```{=tex}
\begin{table}[ht!]
\caption{\label{tab:activation}The name, specification, function, and range for each of the activation functions implemented in the \pkg{RWNN} package.}
\centering
\begin{tabular}{lllc}
\toprule
\textbf{Name} & \textbf{Specification} & \textbf{Function} & \textbf{Range} \\ 
\midrule
Identity & \texttt{identity} & $f(x) = x$ & $(-\infty, \infty)$ \\[5pt]
Bent identity & \texttt{bentidentity} & $f(x) = \dfrac{\sqrt{x^2 + 1} - 1}{2} + x$ & $(-\infty, \infty)$ \\[10pt]
Gaussian & \texttt{gaussian} & $f(x) = \exp(-x^2)$ & $(0, 1]$ \\[7pt]
Hyperbolic tangent & \texttt{tanh} & $\text{tanh}(x) = \dfrac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}$ & $(-1, 1)$ \\[10pt]
Rectified linear unit & \texttt{relu} & $f(x) = \max\{0, x\}$ & $[0, \infty)$ \\[5pt]
Sigmoid & \texttt{sigmoid} & $f(x) = \dfrac{1}{1 + \exp(-x)}$ & $(0, 1)$ \\[10pt]
Sigmoid linear unit & \texttt{silu} & $f(x) = \dfrac{x}{1 + \exp(-x)}$ & $[-0.278..., \infty)$ \\[10pt]
Softsign & \texttt{softsign} & $f(x) = \dfrac{x}{1 + |x|}$ & $(-1, 1)$ \\[10pt]
Softplus & \texttt{softplus} & $f(x) = \ln(1 + x)$ & $(0, \infty)$ \\[5pt] 
SQNL & \texttt{sqnl} & $f(x) = \left\{\begin{array}{ll} -1, & \text{if } x < -2 \\[3pt] x + \frac{x^2}{4}, & \text{if } -2 \leq x < 0 \\[3pt] x - \frac{x^2}{4}, & \text{if } 0 \leq x < 2 \\[3pt] 1, & \text{if } 2 \leq x \end{array} \right.$ & $[-1, 1]$ \\[15pt] 
Squared RBF & \texttt{sqrbf} & $f(x) = \left\{\begin{array}{ll} 1 - \frac{x^2}{2}, & \text{if } |x| \leq 1 \\[3pt] \frac{(2 - |x|)^2}{2}, & \text{if } 1 < |x| < 2 \\[3pt] 0, & \text{if } |x| \geq 2 \end{array} \right.$ & $[0, 1]$ \\[5pt]
\bottomrule
\end{tabular}
\end{table}
```

Thus, if the weights of the hidden-layer can be assumed to be known, then the expected response of an RWNN is identical to that of a linear model. However, before estimating the weights, it it is necessary to determine the weights of the hidden-layer.

### Sampling hidden-weights {#HST}

In RWNN's, the standard approach to determining the weights between in input- and hidden-layer(s), is to sample the weights randomly from a uniform distribution limited to an appropriate interval for the activation function used in the hidden-layer. However, as shown by [@Wang2017], using complete random initialisation might not be the best approach when generating these weights, as it tends to require a large number of neurons in the hidden-layer for the RWNN to have generalisability. Therefore, they introduced two alternatives to the random initialisation: quasi-random numbers, and random orthogonal projections. The three methods have all been implemented in the \pkg{RWNN} package, and are outlined below:

-   **Random initialisation**: The weights between the input- and hidden-layer, $w_{ij}$, are sampled independently from the same distribution, $\mathcal D(\theta)$. The \pkg{RWNN} package is set-up to facilitate the use of any (including user defined) distribution to sample these weights. However, if nothing is specified, it defaults to a uniform distribution on the interval $[-1;1]$, i.e.\ $w_{ij}\sim\mathcal U([-1;1])$ for all $i$ and $j$.
-   **Quasi-random initialisation**: Quasi-random numbers have the distinct advantage, when compared to *true* random numbers, that they will cover the domain of interest faster and more evenly. Thus, when using quasi-random numbers, the space can be more effectively explored, leading to a higher performance using less neurons in the hidden-layer. The \pkg{RWNN} package allows for the use of Halton and Sobol' sequences to generate quasi-random numbers by using the \proglang{R} package: \pkg{randtoolbox} [@randtoolbox]. If nothing is specified, it defaults to creating quasi-random numbers on the cube $[-1;1] \times [-1;1]$.
-   **Random orthogonal initialisation** (default): The idea is similar to that of random initialisation, but instead of using the randomly initialized weights directly, it uses an orthonormal basis of the sampled weights. The method used in the \pkg{RWNN} package corresponds to Algorithm 1 of [@Wang2017].

### Estimating output-weights {#EST}

Given a matrix of features, $D$, (i.e. a matrix where the $n$'th row contains $\boldsymbol d_n^T$) and a vector containing the targets, $\boldsymbol y$, the output-weights $\boldsymbol \beta$ can be found by minimising sum-of-squared errors (SSE): \begin{equation}
    \hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{argmin}} \Big\{ || \boldsymbol{y} - D\boldsymbol{\beta}||_2^2\Big\}. \label{eq:ssq}
\end{equation}

In cases where $N > l$, the solution to this system of equations can be found by the normal equation, as: \begin{equation}
    \hat{\boldsymbol{\beta}}^{(ols)} = (D^T D)^{-1}D^T\boldsymbol{y}. \label{eq:ols}
\end{equation}

However, when the number of columns of $D$ is bigger than the number of observations (i.e. $l >> N$), this optimisation problem becomes ill-posed. In such cases, the two most common approaches, to estimating the output-weights, are Moore-Penrose pseudoinverse and regularisation.

Using Moore-Penrose pseudoinverse [@BjerhammarInv, @PenroseInv], $D^+$, the solution to the optimisation problem is given by: \begin{equation}
    \hat{\boldsymbol \beta}^{(pseudo)} = D^+ \boldsymbol y.
\end{equation}

Regularisation of the SSE, seen in Eq. \eqref{eq:ssq}, is achieved by penalising the size of the output-weights. This penalisation is usually performed using either the $\ell_1$ or $\ell_2$-norm, creating the following optimisation problem: \begin{equation}
    \hat{\boldsymbol{\beta}}^{(reg)} = \underset{\boldsymbol{\beta}}{\text{argmin}} \Big\{ || \boldsymbol{y} - D\boldsymbol{\beta}||_2^2  + \lambda||\boldsymbol{\beta}||_q^q \Big\}, \label{eq:rsse}
\end{equation} where $q \in \{1, 2\}$, and $\lambda$ is a penalisation constant. The penalisation constant should be chosen such that it minimises the out-of-sample error (using e.g. $k$-fold cross-validation during the training process).

Estimating of the weights differs slightly dependent on the choice of $q$:

-   \underline{$\boldsymbol{q = 1}$}:\newline  The optimisation problem in Eq. \eqref{eq:rsse} is equivalent to performing lasso-regression [@SanLasso, @TibLasso]. Unlike ridge-regression ($q = 2$) it is not possible to find a closed form solution to the optimisation problem and, thereby, the lasso estimated output-weights, $\hat{\boldsymbol{\beta}}^{(lasso)}$. However, it has been shown that $\hat{\boldsymbol{\beta}}^{(lasso)}$ can be found using coordinate descent [@CoordLasso].

-   \underline{$\boldsymbol{q = 2}$}:\newline  The optimisation problem in Eq. \eqref{eq:rsse} is equivalent to performing ridge-regression on the concatenated features, instead of the input features, and the solution can be found as [@ridgeReg]: \begin{equation}
    \hat{\boldsymbol \beta}^{(ridge)} = \left(D^TD + \lambda I_{l}\right)^{-1}D^T\boldsymbol y,
    \end{equation} where $I_{l}$ is the identity matrix of size $l$.

## Deep RWNN {#deepRWNN}

Extending FFNN's from a single hidden-layer to $K$ hidden-layers will barely change the governing equation of each neuron; assuming $h_{ni}^{(0)}$ is the $i$'th feature $x_{ni}$ and that the number of neurons in the $k$'th layer is $J^{(k)}$, then the output of the $j$'th neuron in the $k$'th layer for the $n$'th observation, is given by: \begin{equation}
    h_{nj}^{(k)} = f^{(k)}\left(\sum_{i = 1}^{J^{(k - 1)}} w^{(k)}_{ij} h_{ni}^{(k - 1)} + w^{(k)}_{0}\right), \label{eq:hidden2} 
\end{equation} where $f^{(k)}$ is the activation function, $w^{(k)}_0$ is the bias, and $w^{(k)}_{ij}$ is the weight between the $i$'th neuron and the $j$'th neuron in the $k$'th layer for $k = 1, ..., K$. Using vector notation this is simplified to: \begin{equation}
    \boldsymbol h_n^{(k)} = \boldsymbol f^{(k)}\left(W^{(k)} \boldsymbol{h}^{(k)} + w^{(k)}_0 \boldsymbol{1}_{k}\right),
\end{equation} where $\boldsymbol{1}_k$ is a vector of one's of dimension $J^{k}$.

However, when extending RWNN's to multiple hidden layers, two approaches can be taken to predicting the target: (1) use the output of just the last hidden-layer, or (2) concatenate the outputs of all $K$ hidden-layers. The graphical interpretation of the difference between the two approaches can be seen on the left and right-hand sides of Figure \ref{fig:deeprwnn}, respectively.

```{r deeprwnn, fig.show = "hold", fig.pos = "ht!", out.width = "49%", fig.align='center', include = TRUE, fig.cap=c("Graph representation of a deep random weight neural network (RWNN) with functional link between the input-layer, intermediate hidden-layers, and the output-layer."), echo = FALSE}
knitr::include_graphics(c("./Figures/deepRWNN.pdf", "./Figures/deepRWNNalt.pdf"))
```

The two structures depicted in Figure \ref{fig:deeprwnn}, yields four options when constructing a deep RWNN:

-   $\boldsymbol{d}_n = \boldsymbol h_n^{(K)}$

-   $\boldsymbol{d}_n = [(\boldsymbol h_n^{(K)})^T \; \boldsymbol x_n^T]^T$

-   $\boldsymbol{d}_n = [(\boldsymbol h_n^{(1)})^T \; (\boldsymbol h_n^{(2)})^T \; \cdots \; (\boldsymbol h_n^{(K)})^T]^T$

-   $\boldsymbol{d}_n = [(\boldsymbol h_n^{(1)})^T \; (\boldsymbol h_n^{(2)})^T \; \cdots \; (\boldsymbol h_n^{(K)})^T \; \boldsymbol x_n^T]^T$

Given a matrix $D$, with its $n$'th row corresponding to $\boldsymbol{d}_n$, it follows that estimating the output-weights of a deep RWNN is equivalent to that of the RWNN using only a single hidden-layer.

Like the estimation process, sampling the hidden weights is similar to that of RWNN's with a single hidden-layer. However, while the random and random orthogonal initialisation methods can be performed independently on a layer-by-layer basis, the quasi-random initialisation has to continue its quasi-random sequence layer-to-layer (i.e.\ the sequence is not reset between layers).

The \pkg{RWNN} package allows the user to specify the number of neurons and the activation function (found in Table \ref{tab:activation}) in each of the $K$ hidden-layers, as well as which of the four modelling approaches should be taken (when nothing is specified it defaults to the third option).

## Sparse RWNN

While it has been shown that the theoretical properties of RWNNs are similar to FFNNs, due to the random nature of their construction, they typically require (a lot) more neurons to achieve similar results. That is, RWNNs tend to be more inefficient than FFNNs. Therefore, a common approach, when constructing RWNNs, is to create very large networks and then reduce them in size. This size reduction can occur either before, or after, the output parameters are trained. The \pkg{RWNN} package implements two types of reduction: pre-training and network pruning.

### Pre-training



### Network pruning 
-   **Global magnitude**: 
-   **Uniform magnitude**: 
-   **LAMP**: 
-   **APoZ**:
-   **Correlation**:
-   **Relief**: 

## Ensemble methods

### Stacking

### Bagging

### Boosting

### Ensemble Deep RWNN

# Examples {#EX}

# Conclusions {#CON}

# References

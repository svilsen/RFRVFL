---
documentclass: jss
author:
  - name: SÃ¸ren B. Vilsen
    affiliation: |
      | Department of Mathematical Sciences, Aalborg University
    address: |
      | Skjernvej 4A,
      | 9220 Aalborg East
    email: \email{svilsen@math.aau.dk}
    url: people.math.aau.dk/~svilsen
address: Department of Mathematical Sciences, Aalborg University
title:
  formatted: "Random weight neural networks in \\proglang{R}: The \\pkg{RWNN} package"
  plain:     "Random weight neural networks in R: The RWNN package"
  short:     "\\pkg{RWNN}: Random Weight Neural Networks"
abstract: >
  This paper serves as an introduction to the \pkg{RWNN} package. The \pkg{RWNN} package implements random weight neural networks. The methods are implemented using a combination of \proglang{R} and \proglang{C++} offsetting the heavier computation and estimation to \proglang{C++}  through the \pkg{Rcpp} and \pkg{RcppArmadillo} packages. While implementations of random weight neural networks exist other \proglang{R} packages, these focus on the simplest possible variant of the random weight neural network and cover only very specialised use cases of these networks. Besides a general purpose implementation of random weight neural networks, the \pkg{RWNN} package also includes common variants such as deep RWNN and sparse RWNN, as well as ensemble methods using random weight neural networks as the base learner. Furhtermore, the \pkg{RWNN} packages also includes more sophisticated methods for initialising the random weights, as well as methods for pruning both the number of weights and the number of neurons in the network.
keywords:
  formatted: [random weight neural networks, regularisation, ensemble learning, Bayesian computation, "\\proglang{R}", "\\pkg{Rcpp}", "\\pkg{RcppArmadillo}"]
  plain:     [random weight neural networks, regularisation, ensemble learning, Bayesian computation, R, Rcpp, RcppArmadillo]
output:
  rticles::jss_article:
    keep_tex: TRUE
    number_sections: TRUE
    citation_package: "natbib"
    includes:
        in_header: preamble.tex
biblio-style: jss
bibliography: literature.bib
---

```{r setup, include = FALSE}
options(prompt = 'R> ', continue = '+ ')
```

# Introduction

Neural networks, and variants thereof, have seen a massive increase in popularity in recent years. This has largely been due to the flexibility of the neural network architecture, and their accuracy when applied to highly non-linear problems. However, due to the highly non-linear nature of the neural network architecture, estimating the weights of these networks using gradient based optimisation (i.e. back-propagation), can be slow and does not guarantee a globally optimal solution. In order to combat these problems, various simplifications of the feed forward neural network (FFNN) architecture have been proposed, including random weight neural networks (RWNNs). They were first introduced in the early 1990's under the name random vector functional links (RVFL) [@Schmidt1992, @Pao1994], and a simplified version was re-discovered under the name extreme learning machines (ELM) [@Huang2006] in the mid 2000's. The general idea of RWNNs is to keep the randomly assigned weights of the network between the input-layer and the last hidden-layer fixed, and focus on estimating the weights between the last hidden-layer and the output-layer. In the case of regression, this simplification makes estimating the output weights of a RWNN equivalent to estimating the weights of a (regularised) linear model. Theoretically RWNNs show similar universal approximation properties as their FFNN counterparts, i.e. as the number of neurons tends towards infinity the RWNN should be able to approximate any function arbitrarily well (placing only loose assumptions on the activation function). However, practically the number of neurons needed for this approximation to be acceptable may not be feasible for a particular application. Therefore, extensions of RWNNs have been proposed limiting the number of neurons in favour of deeper architecture, as seen in deep RWNN [@Henriquez2018] and ensemble deep RWNN [@Shi2021], or in favour of sparse unsupervised pre-training of the weights, like sparse RWNN [@Zhang2019].

Implementations of RWNNs already exist in \proglang{R} through the packages \pkg{nnfor} [@nnfor] and \pkg{elmNNRcpp} [@elmNNRcpp], both focusing on the simpler ELM architecture. Both implementations allow for a varying number of neurons in the hidden-layer, as well as the specification of the activation function, but are limited to a single hidden-layer with no functional link between the input- and output-layers. The \pkg{nnfor} package was designed for using ELMs to handle time-series data and allows for forecasting at different temporal frequencies using the \pkg{thief} package. Furthermore, it allows for estimation of the output-weights using Moore-Penrose inversion, $\ell_1$-regularisation, and $\ell_2$-regularisation. The \pkg{elmNNRcpp} package is the successor to \pkg{elmNN} package [@elmNN] re-implemented in \proglang{C++} through \pkg{Rcpp} and \pkg{RcppArmadillo} [@Rcpp, @RcppA]. The package is a standard implementation of the ELM architecture for both regression and classification. The package allows the user to specify the leakage of the implemented relu activation function, as well as the tolerance of the Moore-Penrose inversion used to estimate the output-weights.

\pkg{RWNN} is a general purpose implementation of RWNNs in \proglang{R} [@R] focusing on both regression and classification problems. The \pkg{RWNN} package allows the user to create an RWNN of any depth, set the number of neurons and activation functions in each layer, choose the sampling distribution of the randomly assigned weights, and choose whether the output weights should be estimated by either Moore-Penrose inversion, $\ell_1$-regularisation, or $\ell_2$-regularisation. RWNN is implemented in C++ through \pkg{Rcpp} and \pkg{RcppArmadillo}; along with the standard RWNN implementation the following variants have also been included:

-   **ELM** (extreme learning machine) [@Huang2006]: A simplified version of an RWNN without a link between the input and output layer.
-   **deep RWNN** [@Henriquez2018, @Shi2021]: An RWNN with multiple hidden layers, where the output of each hidden-layer is included as features in the model.
-   **sparse RWNN** [@Zhang2019]: Applies sparse auto-encoder ($\ell_1$ regularised) pre-training to reduce the number non-zero weights between the input and the hidden layer (the implementation generalises this concept to allow for both $\ell_1$ and $\ell_2$ regularisation).
-   **ensemble deep RWNN** [@Shi2021]: An extension of deep RWNNs using the output of each hidden layer to create separate RWNNs. These RWNNs are then used to create an ensemble prediction of the target.

Furthermore, the \pkg{RWNN} package also includes general implementations of the following ensemble methods (using RWNNs as base learners):

-   **Stacking**: Stack multiple randomly generated RWNN's, and estimate their contribution to the weighted ensemble prediction using $k$-fold cross-validation.
-   **Bagging** [@Xin2021]: Bootstrap aggregation of RWNN's creates a number of bootstrap samples, sampled with replacement from the training-set. Furthermore, as in random forest, instead of using all features when training each RWNN, a subset of the features can be chosen at random.
-   **Boosting**: Gradient boosting creates a series of RWNN's, where an element, $k$, of the series is trained on the residual of the previous $(k - 1)$ RWNN's. It further allows for manipulation of the learning rate used to improve the generalisation of the boosted model. Note: like the implemented bagging method, the number of features used in each iteration can be chosen at random (also called stochastic gradient boosting).

As RWNN's will inevitably include a lot of randomly generated features, to improve their computational and memory efficiency, the \pkg{RWNN} package also includes methods for pruning the number of weights and neurons using magnitude, mutual information, and Fisher information.

Lastly, the \pkg{RWNN} package also includes a simple method for grid based hyperparameter optimisation, where $k$-fold cross-validation is applied to the training-set to find the hyperparameters yielding the smallest cross-validation error on a user defined grid (similar to the \pkg{tune} function found in the package \pkg{e1071}).

The remainder of the paper is structured as follows: Section \ref{RWNN} introduces the general idea of RWNNs, this is followed by the method of estimating the output weights, and an outline of each of the implemented RWNN variants. The utility of the \pkg{RWNN} package is shown in Section \ref{EX} applying different RWNN variants to a series of examples. Lastly, a conclusion is found in Section \ref{CON}.

# Random Weight Neural Networks {#RWNN}

An RWNN is a simplification of an FFNN, where the weights between the input-layer and the hidden-layers are kept fixed after the randomly initialisation of the network. That is, the weights between the last hidden-layer and the output-layer are the only weights estimated during the training process. Furthermore, an RWNN may include a direct (also called functional) link between the features and the output. When these links are active, the output can be seen as a concatenation of the last hidden-layer and the input-layer (i.e. a concatenation of the both the original and random non-linear transformation of the features). Using this simplification, the estimation of the output-weights simplifies greatly if the activation between the concatenated layer and the output is linear (i.e. the identity function), as estimating of the output-weights becomes equivalent to estimating the weights in (regularised) multiple linear regression. The general structure of an RWNN with a single hidden-layer can be seen in Figure \ref{fig:rwnn}, where the dashed lines are used to indicate the functional link.

```{r rwnn, out.width = "0.6\\linewidth", include = TRUE, fig.pos = "ht!", fig.align = "center", fig.cap=c("Graph representation of a random weight neural network (RWNN) with funcitonal link between the input and the output layer."), echo=FALSE}
knitr::include_graphics("./Figures/RWNN.pdf")
```

## RWNN {#rwnn}

Given a sample of $N$ observations, $\mathcal D = \{(\boldsymbol x_n, y_n)\}_{n = 1}^N$, where $\boldsymbol x_n$ is $p$ dimensional vector of features and $y_n$ is the output of observation $n$, then the output of the $j$'th neuron of the hidden layer: 
\begin{equation}
    h_{nj} = f\left(\sum_{i = 1}^p w_{ij} x_{ni} + w_{0}\right), \label{eq:hidden}
\end{equation} 
where $f$ is the activation function, $w_0$ is the bias, and $w_{ij}$ is the weight between the $i$'th feature and the $j$'th neuron in $n$'th observation. If the hidden-layer contains $J$ neurons, then the vector of transformed features of the hidden layer, denoted $\boldsymbol h_{n}$, can be simplified to: 
\begin{equation} 
    \boldsymbol h_n = \boldsymbol f\left(W \boldsymbol{x}_n + w_0 \boldsymbol{1}_{J}\right), 
\end{equation} 
where $\boldsymbol{1}_{J}$ is a vector of one's of dimension $J$.

Let $\boldsymbol d_n = [\boldsymbol h_n \; \boldsymbol x_n]^T$ be a stacked vector of the features transformed by the hidden-layer, and original features. Furthermore, assuming the activation in the output-layer is linear, then the expected response of the $n$'th observation is: 
\begin{equation}
    \hat{y}_n = \boldsymbol d^T_n \boldsymbol \beta.
\end{equation}

That is, if weights of the hidden-layer can be assumed to be known, then the expected response of an RWNN is identical to that of a linear model. The \pkg{RWNN} package implements three approaches to estimating output-weights, as outlined in Section \ref{EST}, however, before the output-weights can be estimated it is necessary to determine how to find the weights of the hidden-layer.   

### Sampling hidden-weights {#HST}

In an RWNN, the standard approach to determining the weights between in input- and hidden-layer, is to sample the weights randomly from a uniform distribution limited to an appropriate interval for the given activation function. However, as shown by [@Wang2017], using a complete random initialisation might not be the best approach when generating these weights, as it tends to lead to a large number of neurons in the hidden-layer. Therefore, Wang and Liu introduced two alternatives to the random initialisation: quasi-random numbers, and random orthogonal projections. The three methods have all been implemented in the \pkg{RWNN} package, and are outlined below: 

-   **Random initialisation**: The weights between the input- and hidden-layer, $w_{ij}$, are sampled independently from the same distribution, $\mathcal D(\theta)$. The \pkg{RWNN} package is set-up to facilitate the use of any (including user defined) distribution to sample these weights. However, if nothing is specified, it defaults to a uniform distribution on the interval $[-1;1]$, i.e.\ $w_{ij}\sim\mathcal U([-1;1])$ for all $i$ and $j$. 
-   **Quasi-random initialisation**: Quasi-random numbers have the distinct advantage, when compared to *true* random numbers, that they will cover the domain of interest faster and more evenly. Thus, when using quasi-random numbers, the space can be more effectively explored, leading to a higher performance using less neurons in the hidden-layer. The \pkg{RWNN} package allows for the use of Halton and Sobol' sequences to generate quasi-random numbers by using the \proglang{R} package: \pkg{randtoolbox} [@randtoolbox]. If nothing is specified, it defaults to creating quasi-random numbers on the cube $[-1;1] \times [-1;1]$.
-   **Random orthogonal initialisation** (default): The idea is similar to that of random initialisation, but instead of using the randomly initialized weights directly, it uses an orthonormal basis of the sampled weights. The method used in the \pkg{RWNN} package corresponds to Algorithm 1 of [@Wang2017].

### Estimating output-weights {#EST}

Given a concatenated matrix of features, $D$, (i.e. a matrix where the $n$'th row contains $\boldsymbol d_n$) and a vector containing the output, $\boldsymbol y$, the output-weights $\boldsymbol \beta$ can be found by minimising sum-of-squared errors (SSE): 
\begin{equation}
    \hat{\boldsymbol{\beta}} = \underset{\boldsymbol{\beta}}{\text{argmin}} \Big\{ || \boldsymbol{y} - D\boldsymbol{\beta}||_2^2\Big\}. \label{eq:ssq}
\end{equation}

In the case when $N > p + J$, the solution to this system of equations can be found by the normal equation, as: 
\begin{equation}
    \hat{\boldsymbol{\beta}}^{(ols)} = (D^T D)^{-1}D^T\boldsymbol{y}. \label{eq:ols}
\end{equation}

However, when the number of concatenated features (i.e. the number of columns of $D$) is bigger than the number of observations, i.e. $(p + J) >> N$, this optimisation problem becomes ill-posed. In this case, the two most common approaches to estimating the output-weights, are the Moore-Penrose pseudoinverse and regularisation.

Using the Moore-Penrose pseudoinverse [@BjerhammarInv, @PenroseInv], $D^+$, the solution to the optimisation problem is given by:
\begin{equation}
    \hat{\boldsymbol \beta}^{(pseudo)} = D^+ \boldsymbol y.
\end{equation}

In regularisation the SSE, seen in Eq.~\eqref{eq:ssq}, is penalised by the size of the output-weights. This penalisation is usually performed using either the $\ell_1$ or $\ell_2$-norm, creating the following optimisation problem: 
\begin{equation}
    \hat{\boldsymbol{\beta}}^{(reg)} = \underset{\boldsymbol{\beta}}{\text{argmin}} \Big\{ || \boldsymbol{y} - D\boldsymbol{\beta}||_2^2  + \lambda||\boldsymbol{\beta}||_q^q \Big\}, \label{eq:rsse}
\end{equation} 
where $q \in \{1, 2\}$, and $\lambda$ is a penalisation constant. The penalisation constant should be chosen such that it minimises the out-of-sample error (using e.g. $k$-fold cross-validation during the training process): 

-   \underline{\textbf{When} $\boldsymbol{q = 1}$}:\newline 
The optimisation problem in Eq. \eqref{eq:rsse} is equivalent to performing lasso-regression [@SanLasso, @TibLasso]. Unlike ridge-regression ($q = 2$) it is not possible to find a closed form solution to the optimisation problem and, thereby, the lasso estimated output-weights, $\hat{\boldsymbol{\beta}}^{(lasso)}$. However, it has been shown by [@CoordLasso], that $\hat{\boldsymbol{\beta}}^{(lasso)}$ can be found using coordinate descent.

-   \underline{\textbf{When} $\boldsymbol{q = 2}$}:\newline 
The optimisation problem in Eq. \eqref{eq:rsse} is equivalent to performing ridge-regression on the concatenated features, instead of the input features, and the solution can be found as [@ridgeReg]: 
\begin{equation}
    \hat{\boldsymbol \beta}^{(ridge)} = \left(D^TD + \lambda I_{p + J}\right)^{-1}D^T\boldsymbol y,
\end{equation} 
where $I_{p+J}$ is the identity matrix of size $p+J$.

<!--- **NB:** Extreme learning machine's (ELMs) are effectively RWNNs without the functional link and can, therefore, be trained using exactly the same set of equations by setting $\boldsymbol d_n = \boldsymbol h_n$. --->

## Deep RWNN {#deepRWNN}

Extending an RWNN with a single hidden-layer into a network with $K$ hidden-layers can be approached in two slightly different ways: (1) concatenating only the output of the last hidden-layer with the features, or (2) concatenating the outputs of all of the $K$ hidden-layers with the original features. Furthermore, when extending an RWNN with a single hidden-layer to a network with $K$ hidden-layers, the governing equation, seen in Eq. \eqref{eq:hidden}, barely changes. That is, assuming $h_{ni}^{(0)}$ is the $i$'th feature $x_{ni}$ and that the number of neurons in the $k$'th layer is $J^{(k)}$, then the output of the $j$'th neuron in the $k$'th layer for the $n$'th observation, is given by: 
\begin{equation}
    h_{nj}^{(k)} = f^{(k)}\left(\sum_{i = 1}^{J^{(k - 1)}} w^{(k)}_{ij} h_{ni}^{(k - 1)} + w^{(k)}_{0}\right), \label{eq:hidden2} 
\end{equation} 
where $f^{(k)}$ is the activation function, $w^{(k)}_0$ is the bias, and $w^{(k)}_{ij}$ is the weight between the $i$'th neuron and the $j$'th neuron in the $k$'th layer for $k = 1, ..., K$. Using vector notation this is simplified as:
\begin{equation}
    \boldsymbol h_n^{(k)} = \boldsymbol f^{(k)}\left(W^{(k)} \boldsymbol{h}^{(k)} + w^{(k)}_0 \boldsymbol{1}_{k}\right),
\end{equation} 
where $\boldsymbol{1}_k$ is a vector of one's of dimension $J^{k}$.

If follows from the construction of the deep RWNN's that the main difference is in the construction of the concatenated matrix $D$, whos rows can be made as (1) $\boldsymbol{d}_n = [\boldsymbol h_n^{(K)} \; \boldsymbol x_n]^T$, or (2) $\boldsymbol{d}_n = [\boldsymbol h_n^{(1)} \; \boldsymbol h_n^{(2)} \; \cdots \; \boldsymbol h_n^{(K)} \; \boldsymbol x_n]^T$, respectively. The graphical interpretation of the difference between the two approaches can be seen in Figure \ref{fig:deeprwnn} on panels (a) and (b), respectively. Both approaches have been implemented in the \pkg{RWNN} package.

It follows that estimating the output-weights of this deep RWNN, given a matrix of the concatenated features, is equivalent to that of the RWNN with a single hidden-layer. This makes estimating the weights fast and efficient even for deeper structures.

Lastly, the \pkg{RWNN} package allows the user to specify the activation function in each of the $K$ hidden-layers.

```{r deeprwnn, fig.show = "hold", fig.pos = "ht!", out.width = "45%", fig.align='center', include = TRUE, fig.cap=c("Graph representation of a deep random weight neural network (RWNN) with functional link between the input-layer, intermediate hidden-layers, and the output-layer."), echo = FALSE}
knitr::include_graphics(c("./Figures/deepRWNN.pdf", "./Figures/deepRWNNalt.pdf"))
```

## Sparse RWNN

### Pre-trained using an auto-encoder

### Pruning using magnitude

### Pruning using Fisher information

### Pruning using mutual information

## Ensemble methods

### Stacking

### Bagging

### Boosting

## Ensemble Deep RWNN

# Examples {#EX}

# Conclusions {#CON}

# References
